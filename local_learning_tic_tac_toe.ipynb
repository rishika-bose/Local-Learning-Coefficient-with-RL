{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be267fce-104c-4206-95c2-c28c9b62e726",
   "metadata": {},
   "source": [
    "## Tracking local learning coefficient with a tic-tac-toe bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf2b1b-bcb0-49fe-abe0-23df472121b0",
   "metadata": {},
   "source": [
    "This is a project for the SERIMATS 2024 Winter application - developmental interpretability stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b554bed-359f-4696-ada3-c10bf205abc1",
   "metadata": {},
   "source": [
    "Sources and references:\n",
    "- Tic tac toe bot based off this source: \n",
    "- Also referred to https://plainenglish.io/blog/building-a-tic-tac-toe-game-with-reinforcement-learning-in-python\n",
    "- paper on LLC\n",
    "- this post https://www.alignmentforum.org/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong\n",
    "\n",
    "Toatl hours spent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8234d34b-49ee-4600-9079-f237107aaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install devinterp matplotlib seaborn torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50588425-2568-444c-85ac-d27b109d7c10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextwrap\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m operators\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asserts\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\__init__.py:36\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"This module implements operators that AutoGraph overloads.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mNote that \"operator\" is used loosely here, and includes control structures like\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mconditionals and loops, implemented in functional form, using for example\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mclosures for the body.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Naming conventions:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#  * operator names match the name usually used for the respective Python\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#    idiom; examples: for_stmt, list_append\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# subclasses namedtuple and contains any arguments that are only required\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# for some specializations of the operator.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconditional_expressions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_exp\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m for_stmt\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrol_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_stmt\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\conditional_expressions.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Conditional expressions (e.g. the ternary if statement).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensors\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m tf_cond\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_builtins\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\py_builtins.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_assert\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_parsing_ops\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\cond.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond_v2\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util \u001b[38;5;28;01mas\u001b[39;00m util\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors_impl\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m func_graph \u001b[38;5;28;01mas\u001b[39;00m func_graph_module\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m indexed_slices\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m none_tensor  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variable_scope\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_context\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_ops\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809487-af04-45cf-9fa7-42ef2e548690",
   "metadata": {},
   "source": [
    "Tic tac toe game:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdfbca-7aa7-449a-9ca2-c7b3f3fa6a38",
   "metadata": {},
   "source": [
    "def totuple(a):\n",
    "    try:\n",
    "        return tuple(totuple(i) for i in a)\n",
    "    except TypeError:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cdbbd-e29d-4ba0-912d-b9ad3e6eee12",
   "metadata": {},
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.players = [\"X\", \"O\"]\n",
    "        self.current_player = None\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = None\n",
    "        self.winner = None\n",
    "        self.game_over = False\n",
    "\n",
    "    def available_moves(self):\n",
    "        moves = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 0:\n",
    "                    moves.append((i, j))\n",
    "        return totuple(moves) #note changed\n",
    "\n",
    "    def make_move(self, move):\n",
    "        if self.board[move[0]][move[1]] != 0:\n",
    "            return False\n",
    "        self.board[move[0]][move[1]] = self.players.index(self.current_player) + 1\n",
    "        self.check_winner()\n",
    "        self.switch_player()\n",
    "        return True\n",
    "\n",
    "    def switch_player(self):\n",
    "        if self.current_player == self.players[0]:\n",
    "            self.current_player = self.players[1]\n",
    "        else:\n",
    "            self.current_player = self.players[0]\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows\n",
    "        for i in range(3):\n",
    "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
    "                self.winner = self.players[int(self.board[i][0] - 1)]\n",
    "                self.game_over = True\n",
    "        # Check columns\n",
    "        for j in range(3):\n",
    "            if self.board[0][j] == self.board[1][j] == self.board[2][j] != 0:\n",
    "                self.winner = self.players[int(self.board[0][j] - 1)]\n",
    "                self.game_over = True\n",
    "        # Check diagonals\n",
    "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
    "            self.winner = self.players[int(self.board[0][0] - 1)]\n",
    "            self.game_over = True\n",
    "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
    "            self.winner = self.players[int(self.board[0][2] - 1)]\n",
    "            self.game_over = True\n",
    "\n",
    "    def print_board(self):\n",
    "        print(\"-------------\")\n",
    "        for i in range(3):\n",
    "            print(\"|\", end=\" \")\n",
    "            for j in range(3):\n",
    "                print(self.players[int(self.board[i][j] - 1)] if self.board[i][j] != 0 else \" \", end=\" | \")\n",
    "            print()\n",
    "            print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020144d-7c94-4577-a8b2-c062f83ba806",
   "metadata": {},
   "source": [
    "Check that tictactoe game works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fcd7c-39e2-4b55-b25d-a24d7f8b0233",
   "metadata": {},
   "source": [
    "game = TicTacToe()\n",
    "game.current_player = game.players[0]\n",
    "game.print_board()\n",
    "\n",
    "while not game.game_over:\n",
    "    move = input(f\"{game.current_player}'s turn. Enter row and column (e.g. 0 0): \")\n",
    "    move = tuple(map(int, move.split()))\n",
    "    while move not in game.available_moves():\n",
    "        move = input(\"Invalid move. Try again: \")\n",
    "        move = tuple(map(int, move.split()))\n",
    "    game.make_move(move)\n",
    "    game.print_board()\n",
    "\n",
    "if game.winner:\n",
    "    print(f\"{game.winner} wins!\")\n",
    "else:\n",
    "    print(\"It's a tie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb2ed9-0341-4cdf-a37f-26f61c10179c",
   "metadata": {},
   "source": [
    "Implement RL agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d7e6f-bab3-4fd2-91a5-bdb4d5f4ea41",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount_factor):\n",
    "        self.Q = {}\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_Q_value(self, state, action):\n",
    "        # resolving issues with unhashable types:\n",
    "        #print(\"state type is:\", type(state))\n",
    "        #print(\"action type is:\", type(action))\n",
    "        #res_state = list(map(type, state)) \n",
    "        #print(\"The data types of state in order are : \" + str(res_state)) \n",
    "        #res_action = list(map(type, action)) \n",
    "        #print(\"The data types of action in order are : \" + str(res_action)) \n",
    "        t_state = totuple(state)\n",
    "        if (t_state, action) not in self.Q: #check if this works??\n",
    "            self.Q[(t_state, action)] = 0.0\n",
    "        return self.Q[(t_state, action)] \n",
    "\n",
    "    def choose_action(self, state, available_moves):\n",
    "        state = totuple(state)\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)\n",
    "        else:\n",
    "            Q_values = [self.get_Q_value(state, action) for action in available_moves]\n",
    "            max_Q = max(Q_values)\n",
    "            if Q_values.count(max_Q) > 1:\n",
    "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
    "                i = random.choice(best_moves)\n",
    "            else:\n",
    "                i = Q_values.index(max_Q)\n",
    "            return available_moves[i]\n",
    "\n",
    "    def update_Q_value(self, state, action, reward, next_state):\n",
    "        next_Q_values = [self.get_Q_value(next_state, next_action) for next_action in TicTacToe(next_state).available_moves()]\n",
    "        max_next_Q = max(next_Q_values) if next_Q_values else 0.0\n",
    "        self.Q[(state, action)] += self.alpha * (reward + self.discount_factor * max_next_Q - self.Q[(state, action)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30204148-1bff-44f0-a3fa-44c2b4bef93a",
   "metadata": {},
   "source": [
    "Train and test methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2b18f-f7f1-4d39-bc32-d16d159fd73c",
   "metadata": {},
   "source": [
    "def train(num_episodes, alpha, epsilon, discount_factor):\n",
    "    agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
    "    for i in range(num_episodes):\n",
    "        game = TicTacToe()\n",
    "        state = game.board\n",
    "        while not game.game_over:\n",
    "            available_moves = game.available_moves()\n",
    "            action = agent.choose_action(state, available_moves)\n",
    "            next_state, reward = game.make_move(action)\n",
    "            agent.update_Q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630683f-c6f0-43e8-a165-d8a80854bdca",
   "metadata": {},
   "source": [
    "def test(agent, num_games):\n",
    "    num_wins = 0\n",
    "    for i in range(num_games):\n",
    "        state = TicTacToe().board\n",
    "        while not TicTacToe(state).game_over():\n",
    "            if TicTacToe(state).player == 1:\n",
    "                action = agent.choose_action(state, TicTacToe(state).available_moves())\n",
    "            else:\n",
    "                action = random.choice(TicTacToe(state).available_moves())\n",
    "            state, reward = TicTacToe(state).make_move(action)\n",
    "        if reward == 1:\n",
    "            num_wins += 1\n",
    "    return num_wins / num_games * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e892eec-e5ac-48b3-babc-3ceac28172de",
   "metadata": {},
   "source": [
    "Training and testing the bot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19153e5-2ce8-4c1d-a061-b16846fe913d",
   "metadata": {},
   "source": [
    "# Train the Q-learning agent\n",
    "agent = train(num_episodes=10, alpha=0.5, epsilon=0.1, discount_factor=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fcf7a-92ab-4c7d-ba37-1cc349f6731a",
   "metadata": {},
   "source": [
    "\n",
    "# Test the Q-learning agent\n",
    "win_percentage = test(agent, num_games=10)#10000\n",
    "print(\"Win percentage: {:.2f}%\".format(win_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bce3a6-b75d-4f16-82d6-a11a3752dd6d",
   "metadata": {},
   "source": [
    "Bot attempt #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5f06f-f620-4d8d-ba9b-b2e8c84d5d27",
   "metadata": {},
   "source": [
    "Based on tutorial from: https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd27e01-6cd9-495a-b31a-b8a33db6a298",
   "metadata": {},
   "source": [
    "Bot above has issues with setup, may be able to fix but trying another due to time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdec7ff-18bf-4c02-95e3-db318f2e5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b7fb8-c184-455f-a94a-bfe1113a2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e93342-010b-42b5-927f-26336023f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "    \n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS-i-1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(diag_sum1, diag_sum2)\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "    \n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "    \n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "    \n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(0)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0.1)\n",
    "            self.p2.feedReward(0.5)\n",
    "    \n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    def play(self, rounds=100):\n",
    "        for i in range(rounds):\n",
    "            if i%1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "                    \n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "    \n",
    "    # play with human\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c4f0b-2b73-4059-859b-afa86e6f40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "    \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return boardHash\n",
    "    \n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr*(self.decay_gamma*reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        \n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8451a92-777a-489e-b8d7-28eead25683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a5722-74c2-4039-837f-f108966e6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(\"p1\")\n",
    "p2 = Player(\"p2\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"training...\")\n",
    "st.play(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b163b-e5f8-46df-a906-fbf325df11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.savePolicy()\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b87178-ed98-4737-bdf5-eaeca726ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.loadPolicy(\"policy_p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f961052-6c64-4882-8460-8ff05ebb729a",
   "metadata": {},
   "source": [
    "Human vs Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85558a07-8d5b-4bff-ab55-b315ce6587c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(\"computer\", exp_rate=0)\n",
    "p1.loadPolicy(\"policy_p1\")\n",
    "\n",
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.play2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17ccb1-9dc5-41fc-8f57-c2bc74dcbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install devinterp matplotlib transformers torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b9bda-f8db-4ce3-9e7f-04929eb60deb",
   "metadata": {},
   "source": [
    "### Notes on local learning coefficient (LLC) for RL: \n",
    "\n",
    "The estimator for LLC uses the number of samples in its calculation. For supervised learning this is the size of training data; for RL I am assuming it would be the number of training episodes (i.e. rounds of tic tac toe for this bot). I'm not sure about this assumption - more notes on it to follow. Below I try computing LLC based on this assumption.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa7240-9426-4bcf-bfe1-5f361e25d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Literal, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.sampler import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75d82-cd4d-40c7-beee-7b70a457f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to take num. of rounds as param\n",
    "\n",
    "def estimate_learning_coeff_RL(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: Callable,\n",
    "    num_rounds: int = 0, #added this\n",
    "    sampling_method: Type[torch.optim.Optimizer] = SGLD,\n",
    "    optimizer_kwargs: Optional[Dict[str, Union[float, Literal[\"adaptive\"]]]] = None,\n",
    "    num_draws: int = 100,\n",
    "    num_chains: int = 10,\n",
    "    num_burnin_steps: int = 0,\n",
    "    num_steps_bw_draws: int = 1,\n",
    "    cores: int = 1,\n",
    "    seed: Optional[Union[int, List[int]]] = None,\n",
    "    pbar: bool = True,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    verbose: bool = True,\n",
    ") -> float:\n",
    "    trace = sample(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        criterion=criterion,\n",
    "        sampling_method=sampling_method,\n",
    "        optimizer_kwargs=optimizer_kwargs,\n",
    "        num_draws=num_draws,\n",
    "        num_chains=num_chains,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        num_steps_bw_draws=num_steps_bw_draws,\n",
    "        cores=cores,\n",
    "        seed=seed,\n",
    "        pbar=pbar,\n",
    "        device=device,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    baseline_loss = trace.loc[trace[\"chain\"] == 0, \"loss\"].iloc[0]\n",
    "    avg_loss = trace.groupby(\"chain\")[\"loss\"].mean().mean()\n",
    "    num_samples = num_rounds #len(loader.dataset)\n",
    "\n",
    "    return (avg_loss - baseline_loss) * num_samples / np.log(num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2152a21-2317-4b64-a7cb-694574496e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinterp.slt import estimate_learning_coeff\n",
    "import torch.nn as nn\n",
    "\n",
    "model = p1\n",
    "num_rounds = 10\n",
    "criterion = nn.CrossEntropyLoss() #look into this, may change\n",
    "\n",
    "test_llc = estimate_learning_coeff(\n",
    "        model,\n",
    "        num_rounds,\n",
    "        #train_loader,\n",
    "        criterion=criterion,\n",
    "        )\n",
    "\n",
    "print(test_llc)\n",
    "\n",
    "#AttributeError: 'Player' object has no attribute 'to'\n",
    "# Facing issues with bot and given methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44e8a1-d2f5-4593-9b19-a92083eb52e8",
   "metadata": {},
   "source": [
    "Bot 3\n",
    "\n",
    "Source: https://github.com/nestedsoftware/tictac/blob/master/tictac/qneural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff22ab01-d678-4abe-a9b5-cd865674b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training q_values = tensor([0.4891, 0.4739, 0.4953, 0.4377, 0.4838, 0.4530, 0.5147, 0.5072, 0.4520])\n",
      "Training qlearning X vs. random...\n",
      "200/2000 games, using epsilon=0.6...\n",
      "400/2000 games, using epsilon=0.5...\n",
      "600/2000 games, using epsilon=0.4...\n",
      "800/2000 games, using epsilon=0.30000000000000004...\n",
      "1000/2000 games, using epsilon=0.20000000000000004...\n",
      "1200/2000 games, using epsilon=0.10000000000000003...\n",
      "1400/2000 games, using epsilon=2.7755575615628914e-17...\n",
      "1600/2000 games, using epsilon=0...\n",
      "1800/2000 games, using epsilon=0...\n",
      "2000/2000 games, using epsilon=0...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "#import board\n",
    "#import qneural\n",
    "\n",
    "from board import play_random_move, play_games, Board\n",
    "from qneural import (TicTacNet, NetContext, create_qneural_player,\n",
    "                            get_q_values, play_training_games_x,\n",
    "                            play_training_games_o)\n",
    "\n",
    "policy_net = TicTacNet()\n",
    "target_net = TicTacNet()\n",
    "\n",
    "sgd = torch.optim.SGD(policy_net.parameters(), lr=0.1)\n",
    "loss = MSELoss()\n",
    "net_context = NetContext(policy_net, target_net, sgd, loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    board = Board(np.array([1, -1, -1, 0, 1, 1, 0, 0, -1]))\n",
    "    q_values = get_q_values(board, net_context.target_net)\n",
    "    print(f\"Before training q_values = {q_values}\")\n",
    "\n",
    "print(\"Training qlearning X vs. random...\")\n",
    "play_training_games_x(net_context=net_context,\n",
    "                      o_strategies=[play_random_move])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c457f0-fd6e-4c2d-bf74-9a54fac919cd",
   "metadata": {},
   "source": [
    "Need to figure out train_loader var: https://stackoverflow.com/questions/57258323/how-can-i-use-a-pytorch-dataloader-for-reinforcement-learning\n",
    "\n",
    "Looks like https://slm-lab.gitbook.io/slm-lab/analyzing-results/analytics may have some method to store data from RL?\n",
    "\n",
    "Possibly-relevant discussion: https://github.com/Lightning-AI/lightning/issues/713\n",
    "\n",
    "There may be some hacks but not sure if they would make sense w.r.t calculating LLC. Would need to explore this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e0fb15-d39d-4a74-b3ce-7418c457ee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain 0:   0%|                                                                                 | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_rounds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m#change 10 \u001b[39;00m\n\u001b[0;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss() \u001b[38;5;66;03m#look into this, may change\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m test_llc \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_learning_coeff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#train_loader,\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_llc)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\devinterp\\slt\\learning_coeff.py:28\u001b[0m, in \u001b[0;36mestimate_learning_coeff\u001b[1;34m(model, loader, criterion, sampling_method, optimizer_kwargs, num_draws, num_chains, num_burnin_steps, num_steps_bw_draws, cores, seed, pbar, device, verbose)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mestimate_learning_coeff\u001b[39m(\n\u001b[0;32m     13\u001b[0m     model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m     14\u001b[0m     loader: DataLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m---> 28\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_draws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     baseline_loss \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mloc[trace[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     45\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\devinterp\\slt\\sampler.py:157\u001b[0m, in \u001b[0;36msample\u001b[1;34m(model, loader, criterion, sampling_method, optimizer_kwargs, num_draws, num_chains, num_burnin_steps, num_steps_bw_draws, cores, seed, pbar, device, verbose, return_weights)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_chains):\n\u001b[1;32m--> 157\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_sample_single_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    159\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\devinterp\\slt\\sampler.py:81\u001b[0m, in \u001b[0;36m_sample_single_chain\u001b[1;34m(kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample_single_chain\u001b[39m(kwargs):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_single_chain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\devinterp\\slt\\sampler.py:58\u001b[0m, in \u001b[0;36msample_single_chain\u001b[1;34m(ref_model, loader, criterion, num_draws, num_burnin_steps, num_steps_bw_draws, sampling_method, optimizer_kwargs, chain, seed, pbar, verbose, device, return_weights)\u001b[0m\n\u001b[0;32m     52\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[0;32m     53\u001b[0m         iterator, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mnum_steps, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m verbose  \u001b[38;5;66;03m# TODO: Redundant\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (xs, ys) \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     60\u001b[0m     xs, ys \u001b[38;5;241m=\u001b[39m xs\u001b[38;5;241m.\u001b[39mto(device), ys\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "from devinterp.slt import estimate_learning_coeff\n",
    "import torch.nn as nn\n",
    "\n",
    "model = policy_net\n",
    "num_rounds = [0]*10 #change 10 \n",
    "criterion = nn.CrossEntropyLoss() #look into this, may change\n",
    "\n",
    "test_llc = estimate_learning_coeff(\n",
    "        model,\n",
    "        num_rounds,\n",
    "        #train_loader,\n",
    "        criterion=criterion,\n",
    "        )\n",
    "\n",
    "print(test_llc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345dc63-aad9-4031-90e0-160c9f2c491d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
